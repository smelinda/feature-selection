%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction and Motivation}

As the advancements of technology continuously grow, the amount of generated data, as well as the amount of data to be processed, increases at an unprecedented scale \cite{Jagadish:2014}. The process to analyze, make sense and gain insights from such extent and variety of data is widely-known as \textbf{big data analytics} \cite{Beyer:2012}. Real world applications of big data analytics often involve high-dimensional data with large number of features and, sometimes, large number of instances. Consider the variety aspect in big data, different types of data also complicates the analytics process. 

Machine learning are commonly used to gain insights or solve prediction task upon those data. Various real-world applications of machine learning are genomics, biometric recognition, text mining, text classification, etc; In machine learning context, the use of all the available features does not guarantee the best accuracy, training, and classification time of the machine learning task. In fact, having more features could incur greater computational cost and potentially cause over fitting in the machine learning model construction. In order to avoid over fitting, many algorithms use the Occamâ€™s Razor bias \cite{Gamberger:1997} to build a simple model that still achieves some acceptable level of performance on the training data. 
Using this bias, an algorithm often prefers smaller number of predictive feature, with proper combination, that are fully predictive of the class label \cite{Hall:2000}. Too much irrelevant and redundant information or noisy and unreliable data could make the learning process more difficult. Having a subset of features could potentially produce better accuracy, training, and classification time.

\textbf{Feature selection} is an important technique in data preprocessing for machine learning problems. The basic task of feature selection is to reduce the number of features by removing irrelevant or redundant features and noisy data prior to model construction. It is useful to simplify models to make it more understandable, to provide cost-effective training time and to enhance generalization by reducing over fitting. The ultimate goal of feature selection is to speed up a machine learning process, and potentially improve machine learning model performance, such as predictive accuracy and result comprehensibility \cite{Liu:2005}.

Current technology enables real-time analytics to allow faster and more responsive decision-making. This produces a strong need for  huge datasets to be processed and analyzed in a real-time manner. Real-time processing requires algorithm to solve problems as fast as possible. Therefore, the complexity of an algorithm becomes the key concern to choose a certain feature selection algorithm. Certainly, algorithms with linear complexity is an advantage to achieve \textbf{scalability}. However, in some cases we still need to use certain feature selection algorithm although the complexity is not linear, for example: to achieve certain accuracy or to handle specific machine learning task.

Different approaches have been proposed to handle feature selection for large data. An example study proposed a parallel large-scale feature selection approach for logistics regression \cite{Singh:2009}. The approach reaches scalabilty by parallelizing simultaneously over both features and records, which allows quick evaluation of billions of potential features even for very large data sets.

Parallel data processing systems, such as Apache Hadoop \cite{Hadoop:2016}, Apache Spark \cite{Spark:2016}, and Apache Flink \cite{Flink:2016}, provides cost-efficient technology capable of storing and analysing huge datasets with support for arbitrary data types and complex analysis tasks. They used the principles of parallel databases to exploit parallelism in clusters of commodity hardware computers and offer programming models for custom data analysis tasks. MapReduce \cite{Dean:2008} as the programming model underlying Hadoop, was criticized for being a bad choice for complex analysis tasks since many tasks do not map naturally to MapReduce. Spark and Flink address this by generalizing MapReduce to a more expressive programming model, called PACT \cite{Alexandrov:2011}, adding support for complex data flows, iterations, and joins. 

Feature selection ne way is to use the distributed environment to increase processing speed, as a common way for handling big data in general. Parallel feature selection algorithms are needed to run in such setting. 

This thesis focuses on parallel feature selection algorithms to handle large scale data. We focus on classification problem as the machine learning problem.

This chapter introduces the thesis and its purpose in relation to machine learning. Section \ref{ch1:mot} describes the motivation of this thesis and Section \ref{ch1:obj} describes the objectives of this thesis. The outline of this thesis is explained in Section \ref{ch1:out}.

%Researchers conduct studies, in the area of data analytics and machine learning, concerning the feature selection. They focus on improving the efficiency of the feature selection process, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods \cite{Guyon:2003}. Other study focus on coming up with best practices for feature selection \cite{Aggarwal:2011,Guyon:2003}, or describing a feature-selection language and creating optimization methods, such as COLUMBUS \cite{Zhang:2016}.

Studies compared top-performing machine learning algorithms, such as logistic regression, Random Forests and SVMs \cite{Caruana:2008}. However, problem of feasible and efficient computation of feature evaluation still remains as current issues, especially when dealing with large scale problems. 

\section{Thesis Objectives}\label{ch1:obj}
This thesis aims to provide insights over existing feature selection algorithms in distributed machine learning context, especially to tackle large scale data. This begins by listing existing feature selection algorithms and analyze the complexity and performance of each algorithm.
Then, if there is any, existing proposed parallel approach of each algorithm are discussed. An example of feasible feature selection algorithm is then selected. An experimental evaluation over selected feature selection approach aims to test its feasibility when handling large scale data in practice.

The literature study aims to deliver a summary of how feature selection algorithms contribute to the performance in solving the feature selection problem. We use the classification of feature selection algorithms proposed in \cite{Liu:2005} and update the list of the algorithms with the most up-to-date improvements of each algorithm. For each of the category, the study thoroughly describes how each algorithm works and analyzes its scalability and performance from various experiments conducted by other researchers. Then, the study lists down any room of improvement or open problems left for each category that possibly leads to better performance when scaling up the data sets. The description of improvement on the scalability includes the possibility of optimization or the feasibility of parallelizing the feature selection approaches.

The thesis also conducts an experiment to verify the applicability of the selected approaches in a distributed context. This involves a complete pipeline of training, modelling and evaluating the features to examine the quality of the approximate machine learning model using the selected feature selection approach. Selected comparable approaches are implemented in Apache Spark, to allow parallelizing the computation over the training sets and potential features. The implementations are applied to two high-dimensional datasets, the internet ads data sets and the YouTube multiview video games data set, taken from UCI repository \cite{Lichman:2013}. The goal of the experiment is to assess the modelling performance (accuracy) and the scalability when run in a distributed context. Additionally, the experimental evaluation is also useful to identify benefits and drawbacks for the algorithm.

\section{Thesis Overview}\label{ch1:out}

The thesis is organized in five chapters. Chapter 2 provides the background study, which explains the main concepts of feature selection, common feature selection categories, and the scalability analysis for each algorithm. The reason to select a feature selection algorithm that is implemented is also explained in this chapter. Chapter 3 explains the implementation detail of the algorithm in Apache Spark. Chapter 4 explains the experiment settings, such as the chosen dataset, the classification model, and the evaluation over the model after the applying feature selection implementation. Chapter 5 concludes the thesis by listing the advantages, the drawbacks of feature selection algorithms, and the potential area of future works.
